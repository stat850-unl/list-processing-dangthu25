---
title: "Lab: List Processing"
author: "Thu Dang"
format: html
number-sections: true
number-depth: 2
---

::: {.callout-note}
You can see the purpose of this assignment as well as the skills and knowledge you should be using and acquiring, in the [Transparency in Learning and Teaching (TILT)](tilt.qmd) document in this repository.  
The TILT document also contains a checklist for self-reflection that will provide some guidance on how the assignment will be graded.
:::

# Data Source

JSON data files for this assignment were obtained from the TVMaze API for three different Doctor Who series as well as two different spin-offs. 

- Dr. Who [2023-2025](https://www.tvmaze.com/shows/72724/doctor-who)
- Dr. Who [2005-2022](https://www.tvmaze.com/shows/210/doctor-who)
- Dr. Who [1963-1996](https://www.tvmaze.com/shows/766/doctor-who)
- [The Sarah Jane Adventures (2007-2020)](https://www.tvmaze.com/shows/970/the-sarah-jane-adventures)
- [Torchwood (2006-2011)](https://www.tvmaze.com/shows/659/torchwood)
- [Torchwood: Web of Lies (2011)](https://www.tvmaze.com/shows/26694/torchwood-web-of-lies)

# Warming Up

For this portion of the assignment, only work with the canonical Dr. Who files (drwho2023.json, drwho2005.json, drwho1963.json). 

## Parse the file

Add a code chunk that will read each of the JSON files in. 
Store the data in a `drwhoYYYY` object, where `YYYY` is the first year the series  began to air. 
How are the data objects stored?

```{r}
# show files
list.files(pattern = "\\.json$")
```

```{r}
# Loading package
library(jsonlite)

# Read the JSON files 
drwho1963 <- fromJSON("drwho-766.json")
drwho2005 <- fromJSON("drwho-210.json")
drwho2023 <- fromJSON("drwho-72724.json")

# Check structure 
str(drwho2005, max.level = 2)
```

---

> Each drwhoYYYY object is stored as a data frame with nested data frames. Each file has a list with two main parts: The first part shows information like name, rating, schedule, etc. The second part has all episodes as a data frame, but some columns like rating, image, and _links are nested data frames within the main data frame.


## Examining List Data Structures

Create a nested markdown list showing what variables are nested at each level of the JSON file. Include an 'episode' object that is a stand-in for a generic episode (e.g. don't create a list with all 700+ episodes in it, just show what a single episode has). Make sure you use proper markdown formatting to ensure that the lists are rendered properly when you compile your document.

Hint: The `prettify()` function in the R package `jsonlite` will spit out a better-formatted version of a JSON file. 


- drwho2005  
  - id  
  - url  
  - name  
  - season  
  - number  
  - type  
  - airdate  
  - airtime  
  - airstamp  
  - runtime  
  - rating  
    - average  
  - image  
    - medium  
    - original  
  - summary  
  - _links  
    - self  
    - show


Is there any information stored in the list structure that you feel is redundant? If so, why?

> Yes, some information is repeated. For example, some URL information is repeated in multiple places. Or each episode includes its own _links$show information although all episodes belong to the same show.  
> This happens because the JSON file stores full information for each episode instead of separating show details from episode details. 


## Develop A Strategy

Consider what information you would need to examine the structure of Dr. Who episodes over time (show runtime, season length, specials) as well as the ratings, combining information across all three data files. 

Sketch one or more rectangular data tables that look like your expected output. Remember that if you link to an image, you must link to something with a picture extension (`.png`, `.jpg`), and if you reference a file it should be using a local path and you must also add the picture to your git repository. 

----

![Sketch one data tables](drwho_sketch.png)

----

What operations will you need to perform to get the data into a form matching your sketch? Make an ordered list of steps you need to take.

----

1. Read 3 JSON files.

2. Extract the episode information from each file.

3. Unnest columns like `rating`, `image`, and `_links`.

4. Fix data types like dates, numbers.

5. Add a column showing which series the episode belongs to.

6. Combine data from all three series.

7. Check for missing or duplicated records before moving on.


## Implement Your Strategy

Add a code chunk that will convert the JSON files into the table(s) you sketched above. 
Make sure that the resulting tables have the correct variable types (e.g., dates should not be stored as character variables).

Print out the first 5 rows of each table that you create (but no more)!

----

```{r}
# loading package
library(jsonlite)
library(dplyr)
library(tidyr)

# create function to tidy each json file
tidy <- function(path, series_name) {
  fromJSON(path) |>
    unnest_wider(rating, names_sep = "_") |>          # turn to rating_average
    unnest_wider(image, names_sep = "_") |>           # turn to image_medium, image_original
    unnest_wider(`_links`, names_sep = "_") |>        # turn to _links_self, _links_show
    unnest_wider(`_links_self`, names_sep = "_") |>   # turn to _links_self_url
    unnest_wider(`_links_show`, names_sep = "_") |>   # turn to _links_show_url
    mutate(airdate = as.Date(airdate), runtime = as.numeric(runtime),
           rating_average = as.numeric(rating_average),
           series = series_name
    )
}
        
# apply function to all 3 series
drwho1963 <- tidy("drwho-766.json", "Doctor Who (1963)")
drwho2005 <- tidy("drwho-210.json", "Doctor Who (2005)")
drwho2023 <- tidy("drwho-72724.json", "Doctor Who (2023)")

# combine all 
drwho_combine <- bind_rows(drwho1963, drwho2005, drwho2023)

head(drwho_combine, 5)
```
----

> I finished converting the JSON files into tidy tables. I used a function to unnest the nested fields from each JSON file. I unnest columns such as rating, image, and _links using unnest_wider(). Then I changed airdate to a Date and made sure numbers like runtime and rating_average have the right type. Then I combined all three into one data frame drwho_combine.


## Examining Episode Air Dates

Visually represent the length of time between air dates of adjacent episodes within the same season, across all seasons of Dr. Who. You may need to create a factor to indicate which Dr. Who series is indicated, as there will be a Season 1 for each of the series. 
Your plot must have appropriate labels and a title.

----

```{r}
# loading package
library(ggplot2)
library(dplyr)

# calculate airdate gaps
episode_gaps <- drwho_combine |>
  arrange(series, season, airdate) |>
  group_by(series, season) |>
  mutate(gap_days = as.numeric(airdate - lag(airdate))) |>
  ungroup()

episode_gaps_clean <- episode_gaps |>
  filter(!is.na(gap_days), !is.infinite(gap_days))

head(episode_gaps[, c("season", "number", "name", "airdate", "gap_days")], 10)

# draw plot
ggplot(episode_gaps_clean, aes(x = number, y = gap_days, group = interaction(series, season),
                               color = as.factor(season))) +
      geom_line(alpha = 0.7) +
      geom_point(size = 1.2) +
      facet_wrap(~ series, scales = "free_x") +
      xlab("Episode Number") + ylab("Gap Between Episodes") +
      ggtitle("Time Gaps Between Doctor Who Episodes") +
      labs(color = "Season")
```
----

In 2-3 sentences, explain what conclusions you might draw from the data. What patterns do you notice? Are there data quality issues?

> From the plot, I can see that most episodes in each season aired only a few days apart, less than 10 days. But I still can see there are some very large gaps, like in the 2005 series, showing some delays or breaks. Some outliers or missing values might relate to data quality issues, like missing air dates.

# Timey-Wimey Series and Episodes

## Setting Up

In this section of the assignment, you will work with all of the provided JSON files. 
Use a functional programming approach to read in all of the files and bind them together. 

----

```{r}
library(jsonlite)
library(dplyr)
library(tidyr)
library(purrr)

json_files <- list.files(pattern = "\\.json$")

# read and combine all JSON files 
whoverse <- map_dfr(json_files, fromJSON, .id = "source_file")

head(whoverse, 5)
```
----

Then, use the processing code you wrote for the previous section to perform appropriate data cleaning steps. 
At the end of the chunk, your data should be in a reasonably tidy, rectangular form with appropriate data types. 
Call this rectangular table `whoverse`. 

----

```{r}
# define a function to clean each file
tidy <- function(path) {
  dt <- fromJSON(path)
  dt |>
    unnest_wider(rating, names_sep = "_") |>
    unnest_wider(image, names_sep = "_") |>
    unnest_wider(`_links`, names_sep = "_") |>
    unnest_wider(`_links_self`, names_sep = "_") |>
    unnest_wider(`_links_show`, names_sep = "_") |>
    mutate(airdate = as.Date(airdate), runtime = as.numeric(runtime),                                                     rating_average = as.numeric(rating_average), show_name = gsub("\\.json", "", path))
}

# apply function to all files 
whoverse <- map_dfr(json_files, tidy)

head(whoverse, 5)
```

----


## Air Time

Investigate the air time of the episodes relative to the air date, series, and season.
It may help to know that the [watershed](https://en.wikipedia.org/wiki/Watershed_(broadcasting)) period in the UK is 9:00pm - 5:30am. 
Content that is unsuitable for minors may only be shown during this window.
What conclusions do you draw about the target audience for each show? 

How can you explain any shows in the Dr. Who universe which do not have airtimes provided?

## Another Layer of JSON

Use the show URL (`_links` > `show` > `href`) to read in the JSON file for each show. 
As with scraping, it is important to be polite and not make unnecessary server calls, so pre-process the data to ensure that you only make one server call for each show.
You should use a functional programming approach when reading in these files. 


```{r}
library(lubridate)
library(dplyr)

json_files <- list.files(pattern = "\\.json$")
whoverse <- map_dfr(json_files, fromJSON, .id = "source_file")

# convert airtime to number
whoverse$airtime_hour <- as.numeric(substr(whoverse$airtime, 1, 2))

# create a new column for audience type
whoverse$target_audience <- ifelse(
  is.na(whoverse$airtime_hour), 
  "no airtime",
  ifelse(whoverse$airtime_hour >= 21 | whoverse$airtime_hour < 6, 
         "adult time", 
         "family")
)

# count how many episodes per category
audience_summary <- whoverse |>
  group_by(source_file, target_audience) |>
  summarise(num_episodes = n(), .groups = "drop")

audience_summary
```
----

> Most Doctorwho episodes across the three series were aired before 9 p.m. This suggests the production team wanted to focus on families or children. Only a small number of episodes were aired after 9 p.m and are marked as adult. For example, almost all episodes from Doctor Who (2005) and Doctor Who (2023) were aired earlier in the evening, while some shows that are related to Doctor Who might want to focus on older audiences. Moreover, Torchwood: Web of Lies doesn't have any airtime recorded, maybe because it was released as an online series.

----

Process the JSON files using a functional approach and construct an appropriate table for the combined data you've acquired during this step (no need to join the data with the full `whoverse` episode-level data). 


```{r}
library(jsonlite)
library(dplyr)
library(purrr)
library(tidyr)

# tidy and rename
whoverse <- map_dfr(json_files, tidy) |> 
  rename(show_url = `_links_show_href`)

# get unique show_url 
unique_shows <- unique(whoverse$show_url)
shows_list <- map(unique_shows, fromJSON)

# clean into table
shows_info <- map_dfr(shows_list, function(x) {
  tibble(
    show_id = x$id, show_name = x$name,
    language = x$language, genres = paste(x$genres, collapse = ", "),
    status = x$status, runtime = x$runtime,
    premiered = x$premiered, ended = x$ended,
    rating = x$rating$average, official_site = x$officialSite
  )
})

head(shows_info, 5)
```

----

What keys would you use to join this data with the `whoverse` episode level data? Explain.

> I used the show URLs from the whoverse table to read another layer of JSON data. Each show file contains general information like the show name, language, genres, and average rating. The new table shows_info has one row per show. If I wanted to join it with the main whoverse table, I could use the column show_url in whoverse and id in shows_info because both refer to the same show.


## Explore!

Use the data you've assembled to answer a question you find interesting about this data.
Any graphics you make should have appropriate titles and axis labels. 
Tables should be reasonably concise (e.g. don't show all 900 episodes in a table), generated in a reproducible fashion, and formatted with markdown. 
Any results (graphics, tables, models) should be explained with at least 2-3 sentences. 

If you're stuck, consider examining the frequency of words in the episode descriptions across different series or seasons. Or, look at the episode guest cast by appending `/guestcast/` to the episode URL and see whether there are common guests across different seasons. 

----

**Question:** Which series has the highest average episode rating?

----

Code goes here -- once you output a result, you should explain it using markdown text, and then start a new code chunk to continue your exploration. 

```{r}
library(dplyr)
library(ggplot2)


rating_summary <- whoverse |>
  group_by(show_name) |> 
  summarise(
    avg_rating = mean(rating_average, na.rm = TRUE),
    num_episodes = n(),
    .groups = "drop"
  )

# Draw plot
ggplot(rating_summary, aes(x = show_name, y = avg_rating, fill = show_name)) +
  geom_col(width = 0.5, show.legend = FALSE) +
  xlab("Series") + ylab("Average Rating") +
  ggtitle("Average Episode Ratings by Series") 
```
----

> From the plot, I can see that Doctor Who (1963-1966) has the highest average episode rating. Meanwhile, Torchwood: Web of Lies has the lowest average rating, maybe because of its short length.Most other shows have similar ratings around 7.5-7.8, which showing consistent audience interest.
